{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Daten vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden die Daten aus den einzelnen .txt Dateien ausgelesen und in ein gemeinsames Dataframe gepackt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Einlesen der ursprünglichen Daten...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Timestamp', 'ID', 'X', 'Y']\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# durch jede .txt Datei im Ordner 'Data' iterieren, Daten einlesen und ans Dataframe anhängen\n",
    "for filename in os.listdir(\"Data/raw\"): \n",
    "    if filename.endswith(\".txt\"):\n",
    "        tmp_df = pd.read_csv(\"Data/raw/\" + filename, sep=\" \", header=None)\n",
    "        tmp_df.columns = column_names \n",
    "        df = df.append(tmp_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Vereinheitlichen von IDs und Timestamps | Deltas zwischen zwei Zeitpunkten berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden die Daten aufbereitet, damit man besser mit ihnen arbeiten kann. Darunter fällt das **Vergeben einer einzigartigen ID für jeden Datenpunkt** (1 Datenpunkt besteht aus 20 Zeitpunkten), sowie das **Vereinheitlichen der Timestamps**. Da für uns nur die Reihenfolge der einzelnen Zeitpunkte von Interesse ist, der Abstand zwischen diesen auch immer gleich ist, können wir den Zeitpunkten die Werte 1-20 zuordnen (wobei zu Zeitpunkt 1 das erste mal x und y Wert erfasst wurde und bei Zeitpunkt 20 das letzte mal). </br>\n",
    "Außerdem werden in diesem Schritt für die X- und Y-Positionen die **relativen Änderungen (Deltas) zwischen zwei Zeitpunkten berechnet** und in den separaten Spalten 'dx' und 'dy' abgelegt. Die Delta-Werte bilden im Weiteren Verlauf unsere Features, d. h. das später trainierte neuronale Netz sagt uns zukünftig erwartete Delta-Positionen für eine gewisse Anzahl an gegebenen Delta-Werten voraus, wodurch sich dann wiederum die zukünftigen Positionen bestimmen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVereinheitlichen von IDs und Timestamps und berechnen der Deltas zwischen zwei Zeitpunkten...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_id = 0\n",
    "prev_x = 0\n",
    "prev_y = 0\n",
    "for i, row in df.iterrows():\n",
    "    if i % 20 == 0:\n",
    "        curr_id += 1 # alle 20 Zeitpunkte erhöht sich die ID\n",
    "        prev_x = row.X\n",
    "        prev_y = row.Y\n",
    "    df.at[i, 'ID'] = curr_id\n",
    "    df.at[i, 'Timestamp'] = (i % 20) + 1\n",
    "    dx = row.X - prev_x # Delta x berechnen\n",
    "    dy = row.Y - prev_y # Delta y berechnen\n",
    "    prev_x = row.X\n",
    "    prev_y = row.Y\n",
    "    df.at[i, 'dx'] = round(dx, 3)\n",
    "    df.at[i, 'dy'] = round(dy, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Aufteilen der Daten in Trainings-, Test- und Validierungsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend werden die Daten in **Test- (80%), Trainings- und Validierungsdaten (je 10%)** aufgeteilt und jeweils als eigene CSV-Datei abgespeichert. Wurden dieser Split bereits schon einmal durchgeführt, so werden die Daten nicht erneut aufgeteilt, sondern einfach die bereits existierenden CSV-Dateien eingelesen. \n",
    "\n",
    "*Anmerkung: Will man das neuronale Netz mit einer neuen Datenaufteilung trainieren, validieren und testen, so müssen einfach die drei CSV-Dateien aus dem Ordner \"Data/datasplits/\" entfernt werden, damit diese bei erneutem Programmdurchlauf neu erzeugt werden.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAufteilen der Daten in Trainings- Validations- und Testdaten...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = np.array(df.ID.unique()) # alle IDs holen\n",
    "np.random.shuffle(ids) # IDs zufällig durchmischen\n",
    "\n",
    "test_percentage = valid_percentage = 0.1 # Testdaten und Validierungsdaten je 10%, der Rest sind Trainingsdaten\n",
    "test_size = int(test_percentage * len(ids))\n",
    "valid_size = int(valid_percentage * len(ids))\n",
    "test_ids, valid_ids, train_ids = ids[:test_size], ids[test_size:test_size + valid_size], ids[test_size + valid_size:]\n",
    "\n",
    "df_train = df.copy()\n",
    "for id in np.concatenate([test_ids, valid_ids]): # alle Datenpunkte der Test- und Validierungsdaten entfernen --> übrig bleiben die Trainingsdaten\n",
    "    df_train = df_train.drop(df_train[df_train.ID == id].index) # --> drop- Methode ist schneller als die append Methode\n",
    "df_test = df.copy()\n",
    "for id in np.concatenate([train_ids, valid_ids]): # alle Datenpunkte der Trainings- und Validierungsdaten entfernen --> übrig bleiben die Testdaten\n",
    "    df_test = df_test.drop(df_test[df_test.ID == id].index)\n",
    "df_valid = df.copy()\n",
    "for id in np.concatenate([test_ids, train_ids]): # alle Datenpunkte der Test- und Trainingsdaten entfernen --> übrig bleiben die Validierungsdaten\n",
    "    df_valid = df_valid.drop(df_valid[df_valid.ID == id].index)\n",
    "df_train.to_csv(path_train, sep=',', index = False)\n",
    "df_test.to_csv(path_test, sep=',', index = False)\n",
    "df_valid.to_csv(path_valid, sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainings- und Testdaten wurden aufgeteilt und sind nun im Ordner 'Data/datasplits' wiederzufinden!\")\n",
    "print(\"Jetzt kanns losgehen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
