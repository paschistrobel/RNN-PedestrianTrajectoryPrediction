{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f416af19-e03e-4929-a970-351b7f2a2b8d",
   "metadata": {},
   "source": [
    "# Daten Vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae9558-ef24-4557-8177-3b3630190092",
   "metadata": {},
   "source": [
    "Im ersten Schritt werden die Daten aus den einzelnen .txt Dateien ausgelesen und in ein gemeinsames Dataframe gepackt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f892a-85e6-47f7-af50-0f6731b7b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Einlesen der ursprünglichen Daten...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a04faa-8a2f-4519-8643-6a59adf322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Timestamp', 'ID', 'X', 'Y']\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "# durch jede .txt Datei im Ordner 'Data' iterieren, Daten einlesen und ans Dataframe anhängen\n",
    "for filename in os.listdir(\"Data/raw\"): \n",
    "    if filename.endswith(\".txt\"):\n",
    "        tmp_df = pd.read_csv(\"Data/raw/\" + filename, sep=\" \", header=None)\n",
    "        tmp_df.columns = column_names \n",
    "        df = df.append(tmp_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e47c93-cb87-441b-b9e4-15c9c27686ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d4cea-bcee-4acb-ae38-0eac11a8060a",
   "metadata": {},
   "source": [
    "## 1.1 Vereinheitlichen von IDs und Timestamps | Deltas zwischen zwei Zeitpunkten berechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a197f-35c3-4b4d-a256-f45c50edde47",
   "metadata": {},
   "source": [
    "In diesem Schritt werden die Daten vorverarbeitet. Darunter fällt das vergeben einer einzigartigen ID für jeden Datenpunkt (1 Datenpunkt besteht aus 20 Zeitpunkten), sowie das Vereinheitlichen der Timestamps. Da für uns nur die Reihenfolge der einzelnen Zeitpunkte von Interesse ist, der Abstand zwischen diesen auch immer gleich ist, können wir den Zeitpunkten die Werte 1-20 zuordnen (wobei zu Zeitpunkt 1 das erste mal x und y Wert erfasst wurde und bei Zeitpunkt 20 das letzte mal). </br>\n",
    "Außerdem werden in diesem Schritt für die X- und Y-Werte die relativen Änderungen zwischen zwei Zeitpunkten berechnet und den separaten Spalten 'dx' und 'dy' abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc443ad-6d8c-4ce7-99a2-ae0c6e2ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVereinheitlichen von IDs und Timestamps und berechnen der Deltas zwischen zwei Zeipunkten...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7de0e-cefb-4f3c-bc69-368bb9911f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_id = 0\n",
    "prev_x = 0\n",
    "prev_y = 0\n",
    "for i, row in df.iterrows():\n",
    "    if i % 20 == 0:\n",
    "        curr_id += 1 # alle 20 Zeitpunkte erhöht sich die ID\n",
    "        prev_x = row.X\n",
    "        prev_y = row.Y\n",
    "    df.at[i, 'ID'] = curr_id\n",
    "    df.at[i, 'Timestamp'] = (i % 20) + 1\n",
    "    dx = row.X - prev_x\n",
    "    dy = row.Y - prev_y\n",
    "    prev_x = row.X\n",
    "    prev_y = row.Y\n",
    "    df.at[i, 'dx'] = round(dx, 3)\n",
    "    df.at[i, 'dy'] = round(dy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d4409-e1c3-45da-81be-5c74d602c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396f07b-36ad-4bad-a3d8-803bafb58fa5",
   "metadata": {},
   "source": [
    "## 1.2 Aufteilen der Daten in Trainings-, Test- und Validierungsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c6722-58af-46e1-9c9f-2580eea09ff3",
   "metadata": {},
   "source": [
    "Um **Data Leakage** zu vermeiden, werden die Daten zuerst in Trainingsdaten und Testdaten aufgeteilt und dann für jede separat ein Scaler verwendet, um die Daten zu normalisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a7514-79a7-4cad-a36e-764fc429d48b",
   "metadata": {},
   "source": [
    "Der Hauptdatensatz (mit allen Daten) wird in die drei einzelnen Datensätze aufgeteilt und diese als .csv Dateien abgespeichert. Wurden dieser Split bereits schon einmal durchgeführt, so werden die bereits existierenden .csv Dateien eingelesen. Will man das neuronale Netz mit neuen Mischungen von Test-, Trainings- und Validierungsdaten probieren, so müssen einfach die drei .csv Dateien aus dem Ordner \"Data/datasplits/\" entfernt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abdd6b-36b4-4d0f-ae52-e51f282541e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAufteilen der Daten in Trainings- und Testdaten...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d34e1-4e06-4338-aecc-1f6765da6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(df.ID.unique()) # alle IDs holen\n",
    "np.random.shuffle(ids) # IDs zufällig durchmischen\n",
    "\n",
    "test_percentage = 0.1 # 10% Testdaten, der Rest Trainingsdaten (wobei von den Trainingsdaten später ein Teil als Validierungsdaten abgespalten wird)\n",
    "test_size = int(test_percentage * len(ids))\n",
    "test_ids, train_ids = ids[:test_size], ids[test_size:]\n",
    "\n",
    "df_train = df.copy()\n",
    "for id in test_ids:\n",
    "    df_train = df_train.drop(df_train[df_train.ID == id].index)\n",
    "df_test = df.copy()\n",
    "for id in train_ids:\n",
    "    df_test = df_test.drop(df_test[df_test.ID == id].index)\n",
    "df_train.to_csv(path_train, sep=',', index = False)\n",
    "df_test.to_csv(path_test, sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd69ce-743e-4208-bd14-a79d71978b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainings- und Testdaten wurden aufgeteilt und sind nun im Ordner 'Data/datasplits' wiederzufinden!\")\n",
    "print(\"Jetzt kanns losgehen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
