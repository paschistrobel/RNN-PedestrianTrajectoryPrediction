{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7ed81-94ed-48c7-af50-931e2214919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe03195-1f85-4d7d-b04b-0c4d6a28f551",
   "metadata": {},
   "source": [
    "# 0. Daten vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aaa2a-e686-48b1-ab9f-fc70d839f811",
   "metadata": {},
   "source": [
    "Bevor wir mit unseren Daten arbeiten können, müssen wir diese erst einmal vorbereiten. Darunter fällt das **Einlesen**, das **Vereinheitlichen von Timestamps und Fußgänger-IDs**, das **Berechnen der Delta-Werte** für die x- und y-Position zwischen zwei Zeitschritten, sowie das **Aufteilen in Trainings- und Testdaten**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fb9c5-e3f7-4b37-b38b-85f09850a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Data/datasplits/traindata.csv\"\n",
    "path_test = \"Data/datasplits/testdata.csv\"\n",
    "\n",
    "# Datenvorbereitung wird nur einmal ausgeführt; Sobald Trainings- und Testdaten als .csv Dateien abgespeichert wurden, werden diese einfach nur eingelesen\n",
    "# Um einen neuen Trainings-/Testsplit zu erhalten, einfach die csv-Dateien aus dem Ordner \"Data/datasplits\" entfernen\n",
    "if not os.path.exists(path_train) or not os.path.exists(path_test):\n",
    "    # Eine genauere Beschreibung, was bei der Vorbereitung der Daten passiert, ist in der Datei \"DataPreparation.ipynb\" selbst zu finden\n",
    "    %run DataPreparation.ipynb\n",
    "\n",
    "# Einlesen von Trainings- und Testdaten\n",
    "df_train = pd.read_csv(path_train, sep=\",\")\n",
    "df_test = pd.read_csv(path_test, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15946f28-a44c-4300-b3f8-5987b1c44169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa7aeb-3c42-4ed6-820c-459299657f81",
   "metadata": {},
   "source": [
    "# 1. Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1755d97-7118-4f04-9167-8de7ed277ad7",
   "metadata": {},
   "source": [
    "Durch die ersten 8 Werte erhalten wir 7 Deltas. Der erste Wert ist immer 0 (da wir ja keinen vorherigen Wert haben, um das Delta zu berechnen). Aus diesem Grund wird der erste Wert entfernt und dann später nur die sieben Delta-Werte in das NN gegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0207d5-8850-4d51-9f24-eeed90b93978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle ersten Zeitschritte werden entfernt (da dx und dy hier eh immer 0 ist)\n",
    "df_train = df_train.drop(df_train[df_train.Timestamp == 1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f53fb0-88db-4dc5-9929-c52d794ed92a",
   "metadata": {},
   "source": [
    "## 1.3 Standardisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34182fd7-a844-4cd5-a430-844fd66c44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainings- und Validierungsdaten Daten skalieren\n",
    "scaler = MinMaxScaler(feature_range = (-1,1)) # = StandardScaler() oder RobustScaler()\n",
    "features = df_train[[\"dx\", \"dy\"]]\n",
    "scaler.fit(features)\n",
    "df_train[[\"dx_scaled\", \"dy_scaled\"]] = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998bd9d-ad82-472f-b7f0-1bbdb1b7f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73311a62-b49a-4387-be85-4db814ec2642",
   "metadata": {},
   "source": [
    "Nun werden die Trainingsdaten so vorbereitet, dass man sie in das RNN einfüttern kann. Dazu brauchen sie die das Format [samples, time steps, features]. Nachdem wir später eine rekursive Single-Step Prediction für den nächsten Delta-Wert (basierend auf den 7 vorhergehenden) durchführen werden, muss man die Daten zusätzlich noch weiter aufteilen, was auch den Vorteil hat, dass man mehr Trainingsdaten hat. Jeder Datenpunkt wird in weitere Datenpunkte aufgeteilt, bestehend aus 7 Werten und der 8e Wert ist dann die erwartete Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308aad9d-4ceb-4f50-ab3f-4f31a8c16697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 7 # Anhand von 7 dx und dy Werten (welche aus 8 Zeitschritten entstanden sind) sagen wir den nächsten Delta-Wert voraus\n",
    "\n",
    "def transform_data_for_rnn(df, sequence_length):\n",
    "    ids = np.array(df.ID.unique()) # alle (20 Timestep) Sequenzen aus dem dataframe holen, um die dann weiter aufzuteilen\n",
    "    x, y = [], []\n",
    "    for id in ids:\n",
    "        df_current = df[df.ID == id] # einen Datenpunkt mithilfe der ID herausgreifen und diesen dann in weitere Datenpunkte aufteilen\n",
    "        feature_data = np.array(df_current[['dx_scaled', 'dy_scaled']])\n",
    "        for i in range(sequence_length, feature_data.shape[0]):\n",
    "            x.append(feature_data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "            y.append(feature_data[i, :]) #contains the prediction values for validation (3rd column = Close),  for single-step prediction\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = transform_data_for_rnn(df_train, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718b3fa-3d42-4caf-b9e4-6e575dce8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27504b54-38e6-4144-bebd-9d1385563ea5",
   "metadata": {},
   "source": [
    "# 2. Neuronales Netz erstellen und trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5810981-7066-4682-85d8-48351bedc7bb",
   "metadata": {},
   "source": [
    "Zunächst erstellen wir unser NN mit all seinen Layern und Eigenschaften."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20ce6c-8671-4280-9dd4-94418a1cebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input shape = 7 (Timeseries) * 2 (features, dx, dy)\n",
    "model.add(LSTM(units = 14, dropout = 0.2, return_sequences = False, input_shape = (x_train.shape[1], x_train.shape[2]))) \n",
    "#model.add(LSTM(units = 14, dropout = 0.2, return_sequences = False)) \n",
    "#model.add(Dense(units = 7))\n",
    "model.add(Dense(units = 2)) # default activation function = 'linear'\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea6a1b-519d-4fa7-b951-f10284afdaee",
   "metadata": {},
   "source": [
    "Jetzt wird das Model konfiguriert und mithilfe der Trainingsdaten trainiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9cc43-a15a-47e0-90f9-ab1591da94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = adam_v2.Adam(learning_rate=0.0001) # decay=lr/epochs)\n",
    "model.compile(loss = 'mse', optimizer = opt)#, metrics = [RootMeanSquaredError()])\n",
    "history = model.fit(x = x_train, y = y_train, epochs = 30, validation_split = 0.1, batch_size = 12, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d0446-913c-4fbb-aa71-53237e7b1444",
   "metadata": {},
   "source": [
    "Der Verlauf der Loss-Funktion sieht folgendermaßen aus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27893986-f0c5-4638-851f-39db6ef36d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_loss(history, epochs):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), sharex=True)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Model loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "    plt.legend([\"Training\", \"Validation\"], loc=\"best\")\n",
    "    plt.grid()\n",
    "    print(history.history.keys())\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_metric(history, epochs):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), sharex=True)\n",
    "    plt.plot(history.history['root_mean_squared_error'])\n",
    "    plt.plot(history.history[\"val_root_mean_squared_error\"])\n",
    "    plt.title(\"Mean Squared Error\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "    plt.legend([\"Training_MSE\", \"Validation_MSE\"], loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74b195-1e41-4d69-87e8-f6094af6dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_loss(history, 5)\n",
    "#plot_model_metric(history, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76308037-7091-458d-8091-7ab4c277ea3e",
   "metadata": {},
   "source": [
    "# 3. Vorhersage von zukünftigen Deltas/ Positionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473de022-c523-4b80-b1be-592138941037",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestep = df_test[df_test.ID == 5] # Zeitschritt mit aktueller ID holen\n",
    "d = current_timestep.loc[(current_timestep.Timestamp > 1) & (current_timestep.Timestamp < 9)] # Startdeltas extrahieren (2-8)\n",
    "print(d)\n",
    "#prediction_values = np.array(d[['dx', 'dy']]) \n",
    "prediction_values = np.array(scaler.transform(d[['dx', 'dy']])) # skalieren der Daten + Umwandeln in Numpy-Array\n",
    "print(prediction_values)\n",
    "prediction_values = np.reshape(prediction_values, (1, 7, 2)) # Startdeltas formatieren, sodass man sie ins NN fütter kann\n",
    "print(prediction_values)\n",
    "# model.predict(prediction_values) # neues Delta predicten\n",
    "prediction_values = np.reshape(prediction_values, (7,2))\n",
    "prediction_values = np.delete(prediction_values, 0, axis=0) # ersten Wert der gegebenen Werte entfernen\n",
    "prediction_values = np.append(prediction_values, np.array([[2 , 1.4]]) , axis=0) # predicteten Wert für nächste Vorhersage anhängen\n",
    "print(np.reshape(prediction_values, (1, 7, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb9e80-113f-40f1-8570-5187ce04ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verschiebt die Werte des Arrays um eins nach vorne und hängt den vorhergesagten Delta-Wert hinten an (erster Wert wird entfernt)\n",
    "# Prinzip: aus [1, 2, 3, 4] und [5] wird [2, 3, 4, 5]\n",
    "# current_deltas braucht Shape (1, 7, 2) und predicted_deltas (1, 2)\n",
    "def shift_deltas(current_deltas, predicted_delta):\n",
    "    new_deltas = np.reshape(current_deltas, (7,2))\n",
    "    new_deltas = np.delete(new_deltas, 0, axis=0) # ersten Wert der gegebenen Werte entfernen\n",
    "    new_deltas = np.append(new_deltas, predicted_delta , axis=0) # predicteten Wert für nächste Vorhersage anhängen\n",
    "    return np.array([new_deltas])\n",
    "\n",
    "# given_deltas = start_deltas = die Deltas der ersten 8 Positionen, mit denen die nächsten n Positionen vorhergesagt werden sollen\n",
    "# x_pos = x-Koordinaten Startwert = letzter gemessener x-Wert, bevor die Prädiktion startet\n",
    "# y_pos = y-Koordinaten Startwert = letzter gemessener y-Wert, bevor die Prädiktion startet\n",
    "def predict_next_n_steps(n, given_deltas, x_pos, y_pos):\n",
    "    pred_dx_values = [] # vorhergesagte Delta-x Werte\n",
    "    pred_dy_values = [] # vorhergesagte Delta-y Werte\n",
    "    pred_x_pos = [] # vorhergesagte x Koordinaten\n",
    "    pred_y_pos = [] # vorhergesagte y Koordinaten  \n",
    "    for i in range(0, n): \n",
    "        pred_value = model.predict(given_deltas) # nächsten Wert anhand der aktuellen 7 Deltas vorhersagen\n",
    "        pred_value_unscaled = np.round(scaler.inverse_transform(pred_value), 3) # Ergebnis zurückskalieren, um tatsächliche Delta-Werte zu erhalten\n",
    "        given_deltas = shift_deltas(given_deltas, pred_value) # Vorhergesagten Delta-Wert zur Liste für die nächste Prädiktion hinzufügen\n",
    "        pred_dx = pred_value_unscaled[0, 0] # herausholen des vorhergesagten dx Wertes aus dem Array\n",
    "        pred_dy = pred_value_unscaled[0, 1] # herausholen des vorhergesagten dy Wertes aus dem Array\n",
    "        x_pos = round(x_pos + pred_dx, 3)\n",
    "        y_pos = round(y_pos + pred_dy, 3)\n",
    "        # berechneten Werte für dx, dy, xPos und yPos an die jeweilige Ergebnisliste anhängen\n",
    "        pred_dx_values.append(pred_dx)\n",
    "        pred_dy_values.append(pred_dy)\n",
    "        pred_x_pos.append(x_pos)\n",
    "        pred_y_pos.append(y_pos)\n",
    "    return pred_dx_values, pred_dy_values, pred_x_pos, pred_y_pos\n",
    "\n",
    "pred_dx_values, pred_dy_values, pred_x_pos, pred_y_pos = predict_next_n_steps(12, lala, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d38a4d0b-cafe-419d-85ea-06852c456f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.18006841 -0.21618316]\n",
      "  [-0.18000134 -0.21520474]\n",
      "  [-0.18127557 -0.21618316]\n",
      "  [-0.18000134 -0.21148672]\n",
      "  [-0.18006841 -0.21148672]\n",
      "  [-0.18516531 -0.21148672]\n",
      "  [-0.18000134 -0.21148672]]]\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "forecast_range = 12 # wir wollen die nächsten 12 Positionen vorhersagen\n",
    "\n",
    "predicted_dx_values = []\n",
    "predicted_dy_values = []\n",
    "predicted_x_pos = []\n",
    "predicted_y_pos = []\n",
    "\n",
    "ids =  np.array(df_test.ID.unique())\n",
    "\n",
    "for id in ids: \n",
    "    # Aktuellen Datenpunkt mithilfe der ID auswählen\n",
    "    current_datapoint = df_test[df_test.ID == id]\n",
    "    given_data = current_datapoint.loc[current_datapoint.Timestamp <= 8] # ersten 8 Positionen sind gegeben, die nächsten 12 sollen vorhergesagt werden\n",
    "    \n",
    "    # anhängen der ersten 8 gegebenen Werte an die Trajektorien-Resultat-Listen\n",
    "    predicted_dx_values.extend(given_data['dx'].tolist())\n",
    "    predicted_dy_values.extend(given_data['dy'].tolist())\n",
    "    predicted_x_pos.extend(given_data['X'].tolist())\n",
    "    predicted_y_pos.extend(given_data['Y'].tolist())\n",
    "    \n",
    "    # von den gegebenen 8 Punkten die 7 Deltas extrahieren und basierend auf diesen die Vorhersagen machen\n",
    "    df_train.drop(df_train[df_train.Timestamp == 1].index)\n",
    "    given_deltas = given_data.drop(given_data[given_data.Timestamp == 1].index)\n",
    "    given_deltas = np.array(scaler.transform(given_deltas[['dx', 'dy']])) # skalieren der Daten + Umwandeln in Numpy-Array\n",
    "    given_deltas = np.reshape(given_deltas, (1, 7, 2))\n",
    "    \n",
    "    # Positionen des Fußgängers holen, an der er sich am Ende der beobachteten Zeit befindet --> = Startkoordinaten für die Prädiktion\n",
    "    start_x = predicted_x_pos[-1]\n",
    "    start_y = predicted_y_pos[-1]\n",
    "    \n",
    "    # nächsten Positionen vorhersagen\n",
    "    predicted_dx, predicted_dy, pred_x, pred_y = predict_next_n_steps(forecast_range, given_deltas, start_x, start_y)\n",
    "    break\n",
    "\n",
    "# Erstellen Listen als Spalten an das Test-Dataframe anhängen\n",
    "#df_test[\"predicted_dx_values\"] = predicted_dx_values\n",
    "#df_test[\"predicted_dy_values\"] = predicted_dy_values\n",
    "#df_test[\"predicted_x_pos\"] = predicted_x_pos\n",
    "#df_test[\"predicted_y_pos\"] = predicted_y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68834e43-bf42-4f9d-b25f-0cadb709fd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>ID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>lala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.338</td>\n",
       "      <td>17.564</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.262</td>\n",
       "      <td>17.468</td>\n",
       "      <td>0.076</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.185</td>\n",
       "      <td>17.392</td>\n",
       "      <td>0.077</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.127</td>\n",
       "      <td>17.296</td>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.050</td>\n",
       "      <td>17.296</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10795</th>\n",
       "      <td>16</td>\n",
       "      <td>5399</td>\n",
       "      <td>-18.102</td>\n",
       "      <td>19.270</td>\n",
       "      <td>-0.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>17</td>\n",
       "      <td>5399</td>\n",
       "      <td>-18.618</td>\n",
       "      <td>19.270</td>\n",
       "      <td>-0.516</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797</th>\n",
       "      <td>18</td>\n",
       "      <td>5399</td>\n",
       "      <td>-19.068</td>\n",
       "      <td>19.270</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10798</th>\n",
       "      <td>19</td>\n",
       "      <td>5399</td>\n",
       "      <td>-19.494</td>\n",
       "      <td>19.270</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>20</td>\n",
       "      <td>5399</td>\n",
       "      <td>-20.033</td>\n",
       "      <td>19.270</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>0.000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10800 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Timestamp    ID       X       Y     dx     dy  lala\n",
       "0              1     5  -4.338  17.564  0.000  0.000  None\n",
       "1              2     5  -4.262  17.468  0.076 -0.096  None\n",
       "2              3     5  -4.185  17.392  0.077 -0.076  None\n",
       "3              4     5  -4.127  17.296  0.058 -0.096  None\n",
       "4              5     5  -4.050  17.296  0.077  0.000  None\n",
       "...          ...   ...     ...     ...    ...    ...   ...\n",
       "10795         16  5399 -18.102  19.270 -0.449  0.000  None\n",
       "10796         17  5399 -18.618  19.270 -0.516  0.000  None\n",
       "10797         18  5399 -19.068  19.270 -0.450  0.000  None\n",
       "10798         19  5399 -19.494  19.270 -0.426  0.000  None\n",
       "10799         20  5399 -20.033  19.270 -0.539  0.000  None\n",
       "\n",
       "[10800 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792cc0f-002c-4f2b-8176-21fd9299efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    current_timestep = df_test[df_test.ID == id] # Zeitschritt mit aktueller ID holen\n",
    "    deltas = current_timestep.loc[(current_timestep.Timestamp > 1) & (current_timestep.Timestamp < 9)] # Initialen Delta-Werte extrahieren (die ersten 7 Deltas)\n",
    "    \n",
    "    given_deltas = np.array(scaler.transform(d[['dx', 'dy']])) # skalieren der Daten + Umwandeln in Numpy-Array\n",
    "    given_deltas = np.reshape(given_deltas, (1, 7, 2)) # Startdeltas formatieren, sodass man sie ins NN fütter kann\n",
    "    \n",
    "    \n",
    "    for i in range(0, forecast_range):\n",
    "        \n",
    "        predicted_deltas = model.predict(current_timesteps)\n",
    "        # skalierten Wert zu den Vorhersagewerten hinzufügen\n",
    "        predicted_deltas = scaler.inverse_transform(predicted_deltas) # zurückskalieren der Werte, um die eingentlichen Deltas zu erhal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf683565-ae60-46cd-ac3c-a576d5ea1628",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111e69f-06db-4a48-8211-92bbcc9b4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pedestrian_trajectory(x_observed, y_observed, x_pred, y_pred, x_real = np.array([]), y_real = np.array([])):\n",
    "    # Letzten Wert der beobachteten Position an das Array der vorhergesagten Positionen anhängen, sodass im Plot eine zusammenhängende Trajektorie dargestellt werden kann\n",
    "    x_pred = np.insert(x_pred, 0, x_observed[-1], axis=0)\n",
    "    y_pred = np.insert(y_pred, 0, y_observed[-1], axis=0)\n",
    "    if x_real.size != 0 and y_real.size != 0:\n",
    "        x_real = np.insert(x_real, 0, x_observed[-1], axis=0)\n",
    "        y_real = np.insert(y_real, 0, y_observed[-1], axis=0)\n",
    "        plt.plot(x_real, y_real, label = \"Real future positions\", color='green', linestyle='dotted', linewidth = 1,\n",
    "             marker='o', markerfacecolor='green', markersize=5)\n",
    "    # Plotten der vorhergesagten Positionen\n",
    "    plt.plot(x_pred, y_pred, label = \"Predicted future positions\", color='red', linestyle='dotted', linewidth = 1,\n",
    "             marker='o', markerfacecolor='red', markersize=5)\n",
    "    # Plotten der beobachteten Positionen\n",
    "    plt.plot(x_observed, y_observed, label = \"Observed positions\", color='blue', linestyle='dotted', linewidth = 1,\n",
    "             marker='o', markerfacecolor='blue', markersize=5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x - coordinate')\n",
    "    plt.ylabel('y - coordinate')\n",
    "    plt.title('Trajectorie of a single pedestrian')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19ba20-0b32-4dae-aefe-672493b241cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Plot\n",
    "gegeben_x = np.array([1,2,3,4])\n",
    "gegeben_y = np.array([1,1,1,1])\n",
    "pred_x = np.array([5,6,7,8,7,6])\n",
    "pred_y = np.array([1,1,1,1,2,2])\n",
    "real_x = np.array([5,6,7,7])\n",
    "real_y = np.array([2,3,4,4])\n",
    "plot_pedestrian_trajectory(gegeben_x, gegeben_y, pred_x_pos, pred_y_pos, x_real = real_x, y_real = real_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f94cf9-08f8-43d6-9537-e4906cd9f8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08962edb-48c8-4cce-bdb6-c788f8e98857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195faea0-58ab-48b3-bcde-5674d6f54c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4fde2-df9d-461c-a0f2-2ffc67522f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661466df-6279-405b-9744-4359ebbb3350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976ab43-3bb2-4144-ac4a-7775b8dc64e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ebc66-d860-4e9d-b4f8-fdf0d22e4041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1e7b8-92a6-4583-a709-7e00ccdeb731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e140-a33e-4917-a807-d59ec64ac0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9004e7d-f9cb-49f1-85fb-469f53605050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47613e69-f6f3-4d34-bb80-9e17cc9485ea",
   "metadata": {},
   "source": [
    "# Normalize and standardize data\n",
    "You can normalize your dataset using the scikit-learn object MinMaxScaler.\n",
    "\n",
    "Good practice usage with the MinMaxScaler and other rescaling techniques is as follows:\n",
    "\n",
    "Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function,\n",
    "Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function\n",
    "Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n",
    "If needed, the transform can be inverted. This is useful for converting predictions back into their original scale for reporting or plotting. This can be done by calling the inverse_transform() function.\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec596d-3b69-407d-a076-e1121f4564af",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation with validation set\n",
    "\n",
    "In general, when you are doing model selection and testing, your data is divided into three parts, training set, validation set and testing set. You use your training set to train different models, estimate the performance on your validation set, then select the model with optimal performance and test it on your testing set.\n",
    "\n",
    "On the other hand, if you are using K-fold cross-validation to estimate the performance of a model, your data is then divided into K folds, you loop through the K folds and each time use one fold as testing(or validation) set and use the rest (K-1) folds as training set. Then you average across all folds to get the estimated testing performance of your model. This is what the Wikipedia page is referring to.\n",
    "\n",
    "But keep in mind that this is for testing a specific model, if you have multiple candidate models and want to do model-selection as well, you have to select a model only with your training set to avoid this subtle circular logic fallacy. So you further divide your (K-1) folds 'training data' into two parts, one for training and one for validation. This means you do an extra 'cross-validation' first to select the optimal model within the (K-1) folds, and then you test this optimal model on your testing fold. In other words, you are doing a two-level cross-validation, one is the K-fold cross-validation in general, and within each cross-validation loop, there is an extra (K-1)-fold cross-validation for model selection. Then you have what you stated in your question, 'Of the k subsamples one subsample is retained as the validation data, one other subsample is retained as the test data, and k-2 subsamples are used as training data.'\n",
    "https://stats.stackexchange.com/questions/90288/in-k-fold-cross-validation-does-the-training-subsample-include-test-set\n",
    "\n",
    "## Why separate test and validation sets?\n",
    "The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model After assessing the final model on the test set, YOU MUST NOT tune the model any further!\n",
    "https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe78366-4635-4f1d-9a8d-c7500616e4b1",
   "metadata": {},
   "source": [
    "# MeanAbsoluteError und MeanSquaredError Test\n",
    "https://keras.io/api/losses/regression_losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cdfba-56ad-48be-97f8-760e49d9b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\n",
    "y_true = [[0.5, 2]]\n",
    "y_pred = [[0.5, 1.5]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "mae = MeanAbsoluteError()\n",
    "print(mae(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d36bf-bb1e-4dd0-8a64-973e71b88b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
