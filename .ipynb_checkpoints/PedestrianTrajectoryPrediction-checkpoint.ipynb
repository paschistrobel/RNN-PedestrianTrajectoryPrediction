{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a7ed81-94ed-48c7-af50-931e2214919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, CuDNNLSTM, LSTM\n",
    "# CuDNNLSTM unterschied zu LSTM: https://stackoverflow.com/questions/49987261/what-is-the-difference-between-cudnnlstm-and-lstm-in-keras\n",
    "# CuDNN ist schneller, man hat allerdings nicht so viele anpassungsmöglichkeiten\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "from tensorflow.python.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe03195-1f85-4d7d-b04b-0c4d6a28f551",
   "metadata": {},
   "source": [
    "# 0. Daten vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7aaa2a-e686-48b1-ab9f-fc70d839f811",
   "metadata": {},
   "source": [
    "Bevor wir mit unseren Daten arbeiten können, müssen wir diese erst einmal vorbereiten. Darunter fällt das **Einlesen**, das **Vereinheitlichen von Timestamps und Fußgänger-IDs**, das **Berechnen der Delta-Werte** für die x- und y-Position zwischen zwei Zeitschritten, sowie das **Aufteilen in Trainings- und Testdaten**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fb9c5-e3f7-4b37-b38b-85f09850a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Data/datasplits/traindata.csv\"\n",
    "path_test = \"Data/datasplits/testdata.csv\"\n",
    "\n",
    "# Datenvorbereitung wird nur einmal ausgeführt; Sobald Trainings- und Testdaten als .csv Dateien abgespeichert wurden, werden diese einfach nur eingelesen\n",
    "# Um einen neuen Trainings-/Testsplit zu erhalten, einfach die csv-Dateien aus dem Ordner \"Data/datasplits\" entfernen\n",
    "if not os.path.exists(path_train) or not os.path.exists(path_test):\n",
    "    # Eine genauere Beschreibung, was bei der Vorbereitung der Daten passiert, ist in der Datei \"DataPreparation.ipynb\" selbst zu finden\n",
    "    %run DataPreparation.ipynb\n",
    "\n",
    "# Einlesen von Trainings- und Testdaten\n",
    "df_train = pd.read_csv(path_train, sep=\",\")\n",
    "df_test = pd.read_csv(path_test, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15946f28-a44c-4300-b3f8-5987b1c44169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa7aeb-3c42-4ed6-820c-459299657f81",
   "metadata": {},
   "source": [
    "# 1. Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1755d97-7118-4f04-9167-8de7ed277ad7",
   "metadata": {},
   "source": [
    "Durch die ersten 8 Werte erhalten wir 7 Deltas. Der erste Wert ist immer 0 (da wir ja keinen vorherigen Wert haben, um das Delta zu berechnen). Aus diesem Grund wird der erste Wert entfernt und dann später nur die sieben Delta-Werte in das NN gegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0207d5-8850-4d51-9f24-eeed90b93978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle ersten Zeitschritte werden entfernt (da dx und dy hier eh immer 0 ist)\n",
    "df_train = df_train.drop(df_train[df_train.Timestamp == 1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f53fb0-88db-4dc5-9929-c52d794ed92a",
   "metadata": {},
   "source": [
    "## 1.3 Standardisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34182fd7-a844-4cd5-a430-844fd66c44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainings- und Validierungsdaten Daten skalieren\n",
    "# scaler = MinMaxScaler(feature_range = (-1,1))\n",
    "scaler = StandardScaler()\n",
    "# scaler = RobustScaler()\n",
    "features = df_train[[\"dx\", \"dy\"]]\n",
    "scaler.fit(features)\n",
    "df_train[\"dx_scaled\"] = 0 # neue Spalte für skalierte dx-Werte erstellen\n",
    "df_train[\"dy_scaled\"] = 0 # neue Spalte für skalierte dy-Werte erstellen\n",
    "df_train[[\"dx_scaled\", \"dy_scaled\"]] = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998bd9d-ad82-472f-b7f0-1bbdb1b7f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73311a62-b49a-4387-be85-4db814ec2642",
   "metadata": {},
   "source": [
    "Nun werden die Trainingsdaten so vorbereitet, dass man sie in das RNN einfüttern kann. Dazu brauchen sie die das Format [samples, time steps, features]. Nachdem wir später eine rekursive Single-Step Prediction für den nächsten Delta-Wert (basierend auf den 7 vorhergehenden) durchführen werden, muss man die Daten zusätzlich noch weiter aufteilen, was auch den Vorteil hat, dass man mehr Trainingsdaten hat. Jeder Datenpunkt wird in weitere Datenpunkte aufgeteilt, bestehend aus 7 Werten und der 8e Wert ist dann die erwartete Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308aad9d-4ceb-4f50-ab3f-4f31a8c16697",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 7 # Anhand von 7 dx und dy Werten (welche aus 8 Zeitschritten entstanden sind) sagen wir den nächsten Delta-Wert voraus\n",
    "\n",
    "def transform_data_for_rnn(df, sequence_length):\n",
    "    ids = np.array(df.ID.unique()) # alle (20 Timestep) Sequenzen aus dem dataframe holen, um die dann weiter aufzuteilen\n",
    "    x, y = [], []\n",
    "    for id in ids:\n",
    "        df_current = df[df.ID == id] # einen Datenpunkt mithilfe der ID herausgreifen und diesen dann in weitere Datenpunkte aufteilen\n",
    "        feature_data = np.array(df_current[['dx_scaled', 'dy_scaled']])\n",
    "        for i in range(sequence_length, feature_data.shape[0]):\n",
    "            x.append(feature_data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "            y.append(feature_data[i, :]) #contains the prediction values for validation (3rd column = Close),  for single-step prediction\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = transform_data_for_rnn(df_train, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718b3fa-3d42-4caf-b9e4-6e575dce8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca4972-0810-446a-9559-632489b5a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27504b54-38e6-4144-bebd-9d1385563ea5",
   "metadata": {},
   "source": [
    "# 2. Neuronales Netz erstellen und trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5810981-7066-4682-85d8-48351bedc7bb",
   "metadata": {},
   "source": [
    "Zunächst erstellen wir unser NN mit all seinen Layern und Eigenschaften."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e20ce6c-8671-4280-9dd4-94418a1cebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 7, 10)             520       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 14)                1400      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 105       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 16        \n",
      "=================================================================\n",
      "Total params: 2,041\n",
      "Trainable params: 2,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# benötigter Input Shape: (samples, timesteps, features) --> samples-Achse muss nicht angegeben werden, sondern man kann beliebig viele samples einfüttern\n",
    "model.add(LSTM(units = 10, dropout = 0.2, activation = \"relu\", return_sequences = True, input_shape = (x_train.shape[1], x_train.shape[2]))) \n",
    "model.add(LSTM(units = 14, dropout = 0.2, return_sequences = False)) \n",
    "model.add(Dense(units = 7))\n",
    "model.add(Dense(units = 2)) # default activation function = 'linear'\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea6a1b-519d-4fa7-b951-f10284afdaee",
   "metadata": {},
   "source": [
    "Jetzt wird das Model konfiguriert und mithilfe der Trainingsdaten trainiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87c9cc43-a15a-47e0-90f9-ab1591da94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - 72s 20ms/step - loss: 0.3307 - val_loss: 0.1106\n"
     ]
    }
   ],
   "source": [
    "opt = adam_v2.Adam(learning_rate=0.001) # decay=lr/epochs)\n",
    "model.compile(loss = 'mse', optimizer = opt)#, metrics = [RootMeanSquaredError()])\n",
    "history = model.fit(x = x_train, y = y_train, epochs = 1, validation_split = 0.1, batch_size = 16, shuffle = True) # Metrik: MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d0446-913c-4fbb-aa71-53237e7b1444",
   "metadata": {},
   "source": [
    "Der Verlauf der Loss-Funktion sieht folgendermaßen aus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27893986-f0c5-4638-851f-39db6ef36d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_loss(history, epochs):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), sharex=True)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Model loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "    plt.legend([\"Training\", \"Validation\"], loc=\"best\")\n",
    "    plt.grid()\n",
    "    print(history.history.keys())\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_metric(history, epochs):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), sharex=True)\n",
    "    plt.plot(history.history['root_mean_squared_error'])\n",
    "    plt.plot(history.history[\"val_root_mean_squared_error\"])\n",
    "    plt.title(\"Mean Squared Error\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "    plt.legend([\"Training_MSE\", \"Validation_MSE\"], loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74b195-1e41-4d69-87e8-f6094af6dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_loss(history, 5)\n",
    "#plot_model_metric(history, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76308037-7091-458d-8091-7ab4c277ea3e",
   "metadata": {},
   "source": [
    "# 3. Vorhersage von zukünftigen Deltas/ Positionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb9e80-113f-40f1-8570-5187ce04ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verschiebt die Werte des Arrays um eins nach vorne und hängt den vorhergesagten Delta-Wert hinten an (erster Wert wird entfernt)\n",
    "# Prinzip: aus [1, 2, 3, 4] und [5] wird [2, 3, 4, 5]\n",
    "# current_deltas braucht Shape (1, 7, 2) und predicted_deltas (1, 2)\n",
    "def shift_deltas(current_deltas, predicted_delta):\n",
    "    new_deltas = np.reshape(current_deltas, (7,2))\n",
    "    new_deltas = np.delete(new_deltas, 0, axis=0) # ersten Wert der gegebenen Werte entfernen\n",
    "    new_deltas = np.append(new_deltas, predicted_delta , axis=0) # predicteten Wert für nächste Vorhersage anhängen\n",
    "    return np.array([new_deltas])\n",
    "\n",
    "# given_deltas = start_deltas = die Deltas der ersten 8 Positionen, mit denen die nächsten n Positionen vorhergesagt werden sollen\n",
    "# x_pos = x-Koordinaten Startwert = letzter gemessener x-Wert, bevor die Prädiktion startet\n",
    "# y_pos = y-Koordinaten Startwert = letzter gemessener y-Wert, bevor die Prädiktion startet\n",
    "def predict_next_n_steps(n, given_deltas, x_pos, y_pos):\n",
    "    pred_dx_values = [] # vorhergesagte Delta-x Werte\n",
    "    pred_dy_values = [] # vorhergesagte Delta-y Werte\n",
    "    pred_x_pos = [] # vorhergesagte x Koordinaten\n",
    "    pred_y_pos = [] # vorhergesagte y Koordinaten  \n",
    "    for i in range(0, n): \n",
    "        pred_value = model.predict(given_deltas) # nächsten Wert anhand der aktuellen 7 Deltas vorhersagen\n",
    "        pred_value_unscaled = scaler.inverse_transform(pred_value) # Ergebnis zurückskalieren, um tatsächliche Delta-Werte zu erhalten\n",
    "        given_deltas = shift_deltas(given_deltas, pred_value) # Vorhergesagten Delta-Wert zur Liste für die nächste Prädiktion hinzufügen\n",
    "        pred_dx = round(pred_value_unscaled[0, 0], 3) # herausholen des vorhergesagten dx Wertes aus dem Array\n",
    "        pred_dy = round(pred_value_unscaled[0, 1], 3) # herausholen des vorhergesagten dy Wertes aus dem Array        \n",
    "        x_pos = round(x_pos + pred_dx, 3)\n",
    "        y_pos = round(y_pos + pred_dy, 3)\n",
    "        # berechneten Werte für dx, dy, xPos und yPos an die jeweilige Ergebnisliste anhängen\n",
    "        pred_dx_values.append(pred_dx)\n",
    "        pred_dy_values.append(pred_dy)\n",
    "        pred_x_pos.append(x_pos)\n",
    "        pred_y_pos.append(y_pos)\n",
    "    return pred_dx_values, pred_dy_values, pred_x_pos, pred_y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82dddd8-1843-44de-a604-10e098fcb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train[29]\n",
    "print(np.round(scaler.inverse_transform(x), 3))\n",
    "print(model.predict(np.array([x_train[29]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a4d0b-cafe-419d-85ea-06852c456f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_range = 12 # wir wollen die nächsten 12 Positionen vorhersagen\n",
    "\n",
    "predicted_dx_values = []\n",
    "predicted_dy_values = []\n",
    "predicted_x_pos = []\n",
    "predicted_y_pos = []\n",
    "\n",
    "ids_test =  np.array(df_test.ID.unique())\n",
    "progress = 1\n",
    "\n",
    "for id in ids_test: \n",
    "    print(\"Fortschritt: \", progress, \"/\", len(ids_test), end='\\r')\n",
    "    progress += 1\n",
    "    # Aktuellen Datenpunkt mithilfe der ID auswählen\n",
    "    current_datapoint = df_test[df_test.ID == id]\n",
    "    given_data = current_datapoint.loc[current_datapoint.Timestamp <= 8] # ersten 8 Positionen sind gegeben, die nächsten 12 sollen vorhergesagt werden\n",
    "    \n",
    "    # anhängen der ersten 8 gegebenen Werte an die Trajektorien-Resultat-Listen\n",
    "    predicted_dx_values.extend(given_data['dx'].tolist())\n",
    "    predicted_dy_values.extend(given_data['dy'].tolist())\n",
    "    predicted_x_pos.extend(given_data['X'].tolist())\n",
    "    predicted_y_pos.extend(given_data['Y'].tolist())\n",
    "    \n",
    "    # von den gegebenen 8 Punkten die 7 Deltas extrahieren und basierend auf diesen die Vorhersagen machen\n",
    "    df_train.drop(df_train[df_train.Timestamp == 1].index)\n",
    "    given_deltas = given_data.drop(given_data[given_data.Timestamp == 1].index)\n",
    "    given_deltas = np.array(scaler.transform(given_deltas[['dx', 'dy']])) # skalieren der Daten + Umwandeln in Numpy-Array\n",
    "    given_deltas = np.reshape(given_deltas, (1, 7, 2))\n",
    "    \n",
    "    # Positionen des Fußgängers holen, an der er sich am Ende der beobachteten Zeit befindet --> = Startkoordinaten für die Prädiktion\n",
    "    start_x = predicted_x_pos[-1]\n",
    "    start_y = predicted_y_pos[-1]\n",
    "    \n",
    "    # nächsten Positionen vorhersagen\n",
    "    pred_dx, pred_dy, pred_x, pred_y = predict_next_n_steps(forecast_range, given_deltas, start_x, start_y)\n",
    "    \n",
    "    predicted_dx_values.extend(pred_dx)\n",
    "    predicted_dy_values.extend(pred_dy)\n",
    "    predicted_x_pos.extend(pred_x)\n",
    "    predicted_y_pos.extend(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d8881-bfb1-42c7-95e2-6eca183bfba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Listen als Spalten an das Test-Dataframe anhängen\n",
    "df_test[\"predicted_dx\"] = predicted_dx_values\n",
    "df_test[\"predicted_dy\"] = predicted_dy_values\n",
    "df_test[\"predicted_x\"] = predicted_x_pos\n",
    "df_test[\"predicted_y\"] = predicted_y_pos\n",
    "df_test.to_csv(\"test.csv\", sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3e11b-2cf4-4596-8f43-e9a2be62d990",
   "metadata": {},
   "source": [
    "# 4. Berechnen der Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322d808-640e-4e5a-bba6-e8d7c543537c",
   "metadata": {},
   "source": [
    "ADE und FDE lassen sich mit derselben Funktion berechnen. Der einzige Unterschied liegt in den übergebenen Daten für die Berechnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398b555-d075-4759-9b07-1630c33e111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen des Displacement Errors\n",
    "# true_pos und pred_pos = Tatsächliche Positionen bzw. vorhergesagte Positionen | Numpy-Arrays der Form [[x1 y1],[x2 y2], ...]\n",
    "def calc_displacement_error(true_pos, pred_pos):\n",
    "    if (len(true_pos) != len(pred_pos)):\n",
    "        print(\"Listen haben unterschiedliche Länge, Displacement Error kann nicht berechnet werden!\")\n",
    "        return\n",
    "    n = len(true_pos) # Datenanzahl\n",
    "    sum_of_euclid_distances = 0 # Summe der euklidischen Distanzen der Punkte\n",
    "    for i in range (0, n):\n",
    "        sum_of_euclid_distances += np.linalg.norm(true_pos[i] - pred_pos[i])\n",
    "    return sum_of_euclid_distances / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60bfce-4ba2-4aab-b818-ef084ebb2f19",
   "metadata": {},
   "source": [
    "## 4.1 Average Displacement Error (ADE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97dc6d4-f30b-4ea2-8582-5cd165639207",
   "metadata": {},
   "source": [
    "The ADE is the error over all the predicted points and the ground truth points from Tobs to Tpred−1 averaged over all pedestrians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82db6e2-d215-499f-9c0c-99fec6349221",
   "metadata": {},
   "source": [
    "Average Displacement Error (ADE): The average of the root\n",
    "mean squared error (RMSE) between the ground truth and\n",
    "the predicted trajectory position at every time frame for the\n",
    "entire duration of 5 seconds. A lower ADE for a method\n",
    "implies that the method has a lower drift from the ground\n",
    "truth on the average, which is desirable \n",
    "(von https://arxiv.org/pdf/1907.08752.pdf#:~:text=Average%20Displacement%20Error%20(ADE)%3A,entire%20duration%20of%205%20seconds.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10d8ed-55c2-4264-8a63-255268906b50",
   "metadata": {},
   "source": [
    "Beim ADE wird der durchschnittliche Displacement Error für alle vorhergesagten Punkte von allen Fußgängern berechnet. Das bedeutet, wir holen uns alle Daten mit Timestamp > 8 aus dem Dataframe, da alle Positionen der Timestamps 9 bis 20 vorhergesagt wurden (basierend auf den ersten 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8fc13-1aaa-4ae8-985f-541aaf373522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df_test.drop(df_test[df_test.Timestamp < 9].index) # nur die vorhergesagten Daten auswählen | ersten 8 Positionen sind gegeben, die anderen 12 werden vorhergesagt\n",
    "true_positions = np.array(df_predictions([['X', 'Y']])) # Ground Trouth der Positionen\n",
    "predicted_positions = np.array(df_predictions([['predicted_x', 'predicted_y']])) # Vorhergesagte Positionen\n",
    "ade = calc_displacement_error(true_positions, predicted_positions)\n",
    "print(\"Average Displacement Error (ADE): \", round(ade, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cc70d-281c-4327-b2d8-124d8e9e9d44",
   "metadata": {},
   "source": [
    "## 4.2 Final Displacement Error (FDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60128c47-1999-4cba-876b-a0b91a2bb54c",
   "metadata": {},
   "source": [
    "The FDE is the error between the predicted position and the real position at t = Tpred−1.\n",
    "= Summe der euklidischen Distanzen aller finalen Vorhergesagten Positionen vs. ground Trouth positionen geteilt durch die Anzahl der Punkte\n",
    "\n",
    "Für Formel siehe Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405db5d6-d984-4359-a261-8e6ebadd6181",
   "metadata": {},
   "source": [
    "Beim FDE wird für jeden Fußgänger nur der Displacement Error von der letzten vorhergesagte Position (in unserem Fall also zum 20. Timestamp) und zugehöriger Ground Trouth berechnet und der Durchschnitt berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6f86532-de50-4670-8bf4-4f0ee6a0244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_positions = df_test[df_test.Timestamps == 20] # Daten für die finalen Positionen extrahieren \n",
    "true_final_positions = np.array(df_final_positions([['X', 'Y']])) # Ground Trouth der finalen Positionen\n",
    "predicted_final_positions = np.array(df_final_positions([['predicted_x', 'predicted_y']])) # Vorhergesagte finale Positionen\n",
    "fde = calc_displacement_error(true_final_positions, predicted_final_positions)\n",
    "print(\"Final Displacement Error (FDE): \", round(fde, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf683565-ae60-46cd-ac3c-a576d5ea1628",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5635c-f353-4b35-be16-a4719521a6d3",
   "metadata": {},
   "source": [
    "Plotting Funktionen in eigene ipynb Datei verschieben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111e69f-06db-4a48-8211-92bbcc9b4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pedestrian_trajectory(x_observed, y_observed, x_pred, y_pred, x_real = np.array([]), y_real = np.array([])):\n",
    "    # Letzten Wert der beobachteten Position an das Array der vorhergesagten Positionen anhängen, sodass im Plot eine zusammenhängende Trajektorie dargestellt werden kann\n",
    "    x_pred = np.insert(x_pred, 0, x_observed[-1], axis=0)\n",
    "    y_pred = np.insert(y_pred, 0, y_observed[-1], axis=0)\n",
    "    if len(x_real) != 0 and len(y_real) != 0:\n",
    "        x_real = np.insert(x_real, 0, x_observed[-1], axis=0)\n",
    "        y_real = np.insert(y_real, 0, y_observed[-1], axis=0)\n",
    "        plt.plot(x_real, y_real, label = \"Real future positions\", color='green', linestyle='dotted', linewidth = 1,\n",
    "             marker='o', markerfacecolor='green', markersize=5)\n",
    "    # Plotten der vorhergesagten Positionen\n",
    "    plt.plot(x_pred, y_pred, label = \"Predicted future positions\", color='red', linestyle='dotted', linewidth = 1,\n",
    "             marker='o', markerfacecolor='red', markersize=5)\n",
    "    # Plotten der beobachteten Positionen\n",
    "    plt.plot(x_observed, y_observed, label = \"Observed positions\", color='blue', linestyle='dotted', linewidth = 1,\n",
    "             marker='o', markerfacecolor='blue', markersize=5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x - coordinate')\n",
    "    plt.ylabel('y - coordinate')\n",
    "    plt.title('Trajectorie of a single pedestrian')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19ba20-0b32-4dae-aefe-672493b241cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Plot\n",
    "gegeben_x = np.array([1,2,3,4])\n",
    "gegeben_y = np.array([1,1,1,1])\n",
    "pred_x = np.array([5,6,7,8,7,6])\n",
    "pred_y = np.array([1,1,1,1,2,2])\n",
    "real_x = np.array([5,6,7,7])\n",
    "real_y = np.array([2,3,4,4])\n",
    "plot_pedestrian_trajectory(gegeben_x, gegeben_y, pred_x_pos, pred_y_pos, x_real = real_x, y_real = real_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b86de-7a42-4a81-97cc-7b87cf109c79",
   "metadata": {},
   "source": [
    "# Beispiel Visualisierungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f81b8a-1f17-4817-80c6-d4f992390323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 Beispieltrajektorien aus  ausgeben lassen\n",
    "ids = np.array(df_train.ID.unique())\n",
    "ids = ids[0:100]\n",
    "for id in ids:\n",
    "    current_data = df_train[df_train.ID == id]\n",
    "    observed = current_data.loc[current_data.Timestamp <= 8] \n",
    "    predicted = current_data.loc[current_data.Timestamp > 8] \n",
    "    observed_x = observed['X'].tolist()\n",
    "    observed_y = observed['Y'].tolist()\n",
    "    predicted_x = predicted['X'].tolist()\n",
    "    predicted_y = predicted['Y'].tolist()\n",
    "    plot_pedestrian_trajectory(observed_x, observed_y, predicted_x, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5b3eb-9311-4f80-8fda-149eb55386ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 Beispieltrajektorien ausgeben lassen\n",
    "ids = np.array(df_test.ID.unique())\n",
    "ids = ids[0:100]\n",
    "for id in ids:\n",
    "    current_data = df_test[df_test.ID == id]\n",
    "    observed = current_data.loc[current_data.Timestamp <= 8] \n",
    "    predicted = current_data.loc[current_data.Timestamp > 8] \n",
    "    observed_x = observed['X'].tolist()\n",
    "    observed_y = observed['Y'].tolist()\n",
    "    gt_predicted_x = predicted['X'].tolist()\n",
    "    gt_predicted_y = predicted['Y'].tolist()\n",
    "    predicted_x = predicted['predicted_x_pos'].tolist()\n",
    "    predicted_y = predicted['predicted_y_pos'].tolist()\n",
    "    plot_pedestrian_trajectory(observed_x, observed_y, predicted_x, predicted_y, x_real = gt_predicted_x, y_real = gt_predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e140-a33e-4917-a807-d59ec64ac0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9004e7d-f9cb-49f1-85fb-469f53605050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47613e69-f6f3-4d34-bb80-9e17cc9485ea",
   "metadata": {},
   "source": [
    "# Normalize and standardize data\n",
    "You can normalize your dataset using the scikit-learn object MinMaxScaler.\n",
    "\n",
    "Good practice usage with the MinMaxScaler and other rescaling techniques is as follows:\n",
    "\n",
    "Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function,\n",
    "Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function\n",
    "Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n",
    "If needed, the transform can be inverted. This is useful for converting predictions back into their original scale for reporting or plotting. This can be done by calling the inverse_transform() function.\n",
    "https://machinelearningmastery.com/normalize-standardize-time-series-data-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec596d-3b69-407d-a076-e1121f4564af",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation with validation set\n",
    "\n",
    "In general, when you are doing model selection and testing, your data is divided into three parts, training set, validation set and testing set. You use your training set to train different models, estimate the performance on your validation set, then select the model with optimal performance and test it on your testing set.\n",
    "\n",
    "On the other hand, if you are using K-fold cross-validation to estimate the performance of a model, your data is then divided into K folds, you loop through the K folds and each time use one fold as testing(or validation) set and use the rest (K-1) folds as training set. Then you average across all folds to get the estimated testing performance of your model. This is what the Wikipedia page is referring to.\n",
    "\n",
    "But keep in mind that this is for testing a specific model, if you have multiple candidate models and want to do model-selection as well, you have to select a model only with your training set to avoid this subtle circular logic fallacy. So you further divide your (K-1) folds 'training data' into two parts, one for training and one for validation. This means you do an extra 'cross-validation' first to select the optimal model within the (K-1) folds, and then you test this optimal model on your testing fold. In other words, you are doing a two-level cross-validation, one is the K-fold cross-validation in general, and within each cross-validation loop, there is an extra (K-1)-fold cross-validation for model selection. Then you have what you stated in your question, 'Of the k subsamples one subsample is retained as the validation data, one other subsample is retained as the test data, and k-2 subsamples are used as training data.'\n",
    "https://stats.stackexchange.com/questions/90288/in-k-fold-cross-validation-does-the-training-subsample-include-test-set\n",
    "\n",
    "## Why separate test and validation sets?\n",
    "The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model After assessing the final model on the test set, YOU MUST NOT tune the model any further!\n",
    "https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe78366-4635-4f1d-9a8d-c7500616e4b1",
   "metadata": {},
   "source": [
    "# MeanAbsoluteError und MeanSquaredError Test\n",
    "https://keras.io/api/losses/regression_losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cdfba-56ad-48be-97f8-760e49d9b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\n",
    "y_true = [[0.5, 2]]\n",
    "y_pred = [[0.5, 1.5]]\n",
    "# Using 'auto'/'sum_over_batch_size' reduction type.\n",
    "mae = MeanAbsoluteError()\n",
    "print(mae(y_true, y_pred).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d36bf-bb1e-4dd0-8a64-973e71b88b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
