{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "# Unterschied zwischen CuDNNLSTM und LSMT: https://stackoverflow.com/questions/49987261/what-is-the-difference-between-cudnnlstm-and-lstm-in-keras\n",
    "from tensorflow.python.keras.layers import Dense, CuDNNLSTM \n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "from tensorflow.python.keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0737b31",
   "metadata": {},
   "source": [
    "# 0. Daten vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6e980",
   "metadata": {},
   "source": [
    "Bevor wir mit unseren Daten arbeiten können, müssen wir diese erst einmal vorbereiten. Darunter fällt das **Einlesen**, das **Vereinheitlichen von Timestamps und Fußgänger-IDs**, das **Berechnen der Delta-Werte** für die x- und y-Position zwischen zwei Zeitschritten, sowie das **Aufteilen in Trainings-, Validierungs- und Testdaten**.\n",
    "\n",
    "*Anmerkung: Der Code zur Datenvorbereitung ist im separaten Notebook \"DataPreparation.ipynb\" zu finden. Der Code für alle Plotting-Methoden ist im separaten Notebook \"Plotting.ipynb\" zu finden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbdc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"Data/datasplits/traindata.csv\" # Pfad zu den Trainingsdaten\n",
    "path_test = \"Data/datasplits/testdata.csv\" # Pfad zu den Testdaten\n",
    "path_valid = \"Data/datasplits/validationdata.csv\" # Pfad zu den Validierungsdaten\n",
    "\n",
    "# Datenvorbereitung wird nur einmal ausgeführt; Sobald Daten bereits gesplittet und als CSV-Dateien abgespeichert wurden, werden diese einfach nur eingelesen\n",
    "# Um einen neuen Datensplit zu erhalten, einfach die CSV-Dateien aus dem Ordner \"Data/datasplits\" entfernen\n",
    "if not os.path.exists(path_train) or not os.path.exists(path_test) or not os.path.exists(path_valid):\n",
    "    # Eine genauere Beschreibung, was bei der Vorbereitung der Daten passiert, ist in der Datei \"DataPreparation.ipynb\" selbst zu finden\n",
    "    %run DataPreparation.ipynb\n",
    "    \n",
    "# Das Plotting-Notebook enthält alle benötigten Funktionen für die im Folgenden erstellten Plots\n",
    "%run Plotting.ipynb\n",
    "\n",
    "# Einlesen von Trainings- und Testdaten\n",
    "df_train = pd.read_csv(path_train, sep=\",\")\n",
    "df_test = pd.read_csv(path_test, sep=',')\n",
    "df_valid = pd.read_csv(path_valid, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e91770e",
   "metadata": {},
   "source": [
    "Hier ein kurzer Einblick in unsere aufbereiteten Trainingsdaten..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b7da9",
   "metadata": {},
   "source": [
    "# 1. Daten vorverarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26534146",
   "metadata": {},
   "source": [
    "## 1.1 Entfernen redundanter Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e7ce5",
   "metadata": {},
   "source": [
    "Als erstes werden nicht benötigte Daten von unserem Trainingsdatensatz entfernt. In unserem Fall können wir **alle ersten Zeitschritte eines Datenpunktes** (besteht aus insgesamt 20 Zeitschritten) entfernen. Da für die ersten X- und Y- Positionen noch kein Delta-Wert berechnet werden kann, ist dx und dy für den ersten Zeitschritt somit immer = 0 und daher nicht aussagekräftig. \n",
    "\n",
    "Erinnerung: Die eigentliche Aufgabe ist es, mithilfe der ersten 8 Positionsdaten die nächsten 12 vorherzusagen. D. h. aus den ersten 8 Positionen erhalten wir 7 Delta-X und Delta-Y Werte. Und genau mit diesen 7 Delta-Werten werden wir später eine (bzw. mehrere) Vorhersagen treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be221a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen aller ersten Zeitschritte eines Datenpunktes\n",
    "df_train = df_train.drop(df_train[df_train.Timestamp == 1].index)\n",
    "df_valid = df_valid.drop(df_valid[df_valid.Timestamp == 1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a48ef",
   "metadata": {},
   "source": [
    "## 1.2 Standardisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0368d",
   "metadata": {},
   "source": [
    "Als nächstes werden die Trainingsdaten und die Valiedierungsdaten skaliert. Dazu wird der StandardScaler von Scikitlearn verwendet und auf die Trainingsdaten gefittet. \n",
    "\n",
    "*Anmerkung: Um **Data Leakage** zu vermeiden, wurden die Daten zuerst in Trainings-, Test- und Validierungsdaten aufgeteilt. Erst dann wurde ein Scaler erstellt und **nur** auf die Trainingsdaten gefittet. Um die Daten der anderen zwei Datesätze zu skalieren, wird dann der mit den Trainingsdaten gefittete Scaler verwendet.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ea9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalieren der Trainingsdaten\n",
    "scaler = StandardScaler()\n",
    "features = df_train[[\"dx\", \"dy\"]] # Features auswählen, die skaliert werden sollen\n",
    "scaler.fit(features)\n",
    "df_train[\"dx_scaled\"] = 0 # neue Spalte für skalierte dx-Werte erstellen und mit 0 initialisieren\n",
    "df_train[\"dy_scaled\"] = 0 # neue Spalte für skalierte dy-Werte erstellen und mit 0 initialisieren\n",
    "df_train[[\"dx_scaled\", \"dy_scaled\"]] = scaler.transform(features) # dx und dy skalieren und in den entsprechenden Spalten ablegen\n",
    "\n",
    "# Skalieren der Validierungsdaten\n",
    "features = df_valid[[\"dx\", \"dy\"]]\n",
    "df_valid[\"dx_scaled\"] = 0 # neue Spalte für skalierte dx-Werte erstellen und mit 0 initialisieren\n",
    "df_valid[\"dy_scaled\"] = 0 # neue Spalte für skalierte dy-Werte erstellen und mit 0 initialisieren\n",
    "df_valid[[\"dx_scaled\", \"dy_scaled\"]] = scaler.transform(features) # dx und dy skalieren und in den entsprechenden Spalten ablegen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152b948",
   "metadata": {},
   "source": [
    "Hier ein kleines Zwischenupdate, wie unsere Testdaten inzwischen aussehen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf491a",
   "metadata": {},
   "source": [
    "## 1.3 Transformieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24180c",
   "metadata": {},
   "source": [
    "In der aktuellen Form, in der unsere Daten vorliegen, können wir sie allerdings nicht in ein RNN der Keras-Library einfüttern. Das vorgeschriebene Format für die Daten sieht nämlich folgendermaßen aus: **[samples, timesteps, features]**. In unserem Fall ist timesteps = 7 und features = 2, da wir eine Zeitreihe mit 7 Delta-Werten mit je zwei Features (dx und dy) zur Vorhersage nutzen wollen. \n",
    "\n",
    "Außerdem soll die Vorhersage der 12 zukünftigen Positionen **rekursiv** erfolgen. D. h. unser NN gibt uns für 7 gegebene Delta-Werte nur den nächsten vorhergesagten Delta-Wert zurück, welcher dann für die nächste Vorhersage wiederum mit einbezogen wird usw. \n",
    "\n",
    "Das bedeutet, jeder einzelne Datenpunkte unserer Trainingsdaten kann in 12 weitere Trainingssamples aufgeteilt werden. Im folgenden wird diese Transformation für die Trainings- und die Validierungsdaten durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f296823",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 7 # Anhand von 7 dx und dy Werten (welche aus 8 Zeitschritten entstanden sind) soll der nächste Delta-Wert vorausgesagt werden\n",
    "\n",
    "# teilt die Trainingsdaten so auf, dass aus jedem Datenpunkt (bestehend aus 20 Zeitschritten/ bzw. 19, da der erste ja entfernt wurde) 12 \n",
    "# Trainingssamples (jeweils 7 dx und dy und ein erwarteter dx und dy Wert) entstehen und bringt die Trainingsdaten in ein passendes Format für das neuronale Netzt\n",
    "def transform_data_for_rnn(df, sequence_length):\n",
    "    ids = np.array(df.ID.unique())\n",
    "    x, y = [], []\n",
    "    for id in ids: # über jeden einzelnen Datenpunkt iterieren\n",
    "        df_current = df[df.ID == id] # einzelnen Datenpunkt herausgreifen und diesen dann in weitere Datenpunkte aufteilen\n",
    "        feature_data = np.array(df_current[['dx_scaled', 'dy_scaled']])\n",
    "        for i in range(sequence_length, feature_data.shape[0]): # einen Datenpunkt in 12 Samples aufteilen\n",
    "            x.append(feature_data[i-sequence_length:i,:]) # enthält 7 dx und dy Werte für die Prädiktion\n",
    "            y.append(feature_data[i, :]) # enthält den erwarteten dx und dy Wert, der mit den anderen 7 Deltas vorhergesagt werden soll\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = transform_data_for_rnn(df_train, sequence_length) # Trainingsdaten, die in das RNN eingefüttert werden und zugehörige Ground Trouth-Werte\n",
    "x_valid, y_valid = transform_data_for_rnn(df_valid, sequence_length) # Validierungsdaten, die in das RNN eingefüttert werden und zugehörige Ground Trouth-Werte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79cbc4",
   "metadata": {},
   "source": [
    "Die Trainings- und Validierungsdaten haben also nun die gewünschte Form..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cead8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Form der Trainingsdaten: \", x_train.shape, y_train.shape)\n",
    "print(\"Form der Validierungsdaten: \", x_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5608a04",
   "metadata": {},
   "source": [
    "# 2. Neuronales Netz erstellen und trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff8fc9",
   "metadata": {},
   "source": [
    "## 2.1 Erstellen des Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e534c",
   "metadata": {},
   "source": [
    "Nachdem die Daten nun fertig vorverarbeitet sind, kann man sich dem eigentlichen NN widmen. Zunächst wird das neuronale Netz mit all seinen Layern und Parametern erstellt.\n",
    "\n",
    "*Anmerkung: Eine präzisere Erklärung der verwendeten Layer, Regularisierungstechniken etc. wird in der Projektdokumentation gegeben.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02288c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(units = 128, return_sequences = True, input_shape = (x_train.shape[1], x_train.shape[2]))) # standard Aktivierungsfunktion: tanh\n",
    "model.add(CuDNNLSTM(units = 128, return_sequences = True, kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(CuDNNLSTM(units = 64, return_sequences = False, kernel_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(units = 2)) # standard Aktivierungsfunktion: linear\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75f822",
   "metadata": {},
   "source": [
    "## 2.2 Trainieren des Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde759b",
   "metadata": {},
   "source": [
    "Das erstellte Model wird nun kompiliert und mithilfe der Trainingsdaten trainiert. Zusätzlich werden beim Kompilieren die Lernrate, die Anzahl der Epochen, sowie die verwendete Loss-Funktion und Metrik festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a027eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 15\n",
    "batch_size = 10\n",
    "opt = adam_v2.Adam(learning_rate = learning_rate)\n",
    "model.compile(loss = 'mse', optimizer = opt, metrics = ['mae'])\n",
    "history = model.fit(x = x_train, y = y_train, epochs = epochs, validation_data=(x_valid, y_valid), batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7199508",
   "metadata": {},
   "source": [
    "Der Verlauf der Loss-Funktion, sowie die Performance sieht für das Model folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_loss(history, epochs)\n",
    "plot_model_metric(history, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9792fce",
   "metadata": {},
   "source": [
    "# 3. Vorhersage von zukünftigen Deltas/ Positionen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b759d3",
   "metadata": {},
   "source": [
    "Das neuronale Netz ist nun fertig trainiert und kann nun anhand von 7 gegebenen Delta-Werten **einen** nächsten Delta-Wert für x- und y-Koordinate vorhersagen. Die folgenden Methoden ermöglichen es, den vorhergesagten Wert für eine erneute Prädiktion mit einzubeziehen, sodass die n nächsten Deltas vorhergesagt werden können. Außerdem werden mithilfe der Delta-Werte auch gleich die erwarteten X- und Y-Positionen der Fußgänger berechnet, um so die eigentliche Trajektorie bestimmen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b212c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verschiebt die Werte des Arrays um eins nach vorne und hängt den vorhergesagten Delta-Wert hinten an (erster Wert wird entfernt)\n",
    "# Prinzip: aus [1, 2, 3, 4] und [5] wird [2, 3, 4, 5]\n",
    "# current_deltas braucht Shape (1, 7, 2) und predicted_deltas (1, 2)\n",
    "def shift_deltas(current_deltas, predicted_delta):\n",
    "    new_deltas = np.reshape(current_deltas, (7,2))\n",
    "    new_deltas = np.delete(new_deltas, 0, axis=0) # ersten Wert der gegebenen Werte entfernen\n",
    "    new_deltas = np.append(new_deltas, predicted_delta , axis=0) # predicteten Wert für nächste Vorhersage anhängen\n",
    "    return np.array([new_deltas])\n",
    "\n",
    "# Methode, die iterative die nächsten n Positionen bestimmt\n",
    "# given_deltas = start_deltas = die Deltas der ersten 8 Positionen, mit denen die nächsten n Positionen vorhergesagt werden sollen\n",
    "# x_pos = x-Koordinaten Startwert = letzter gemessener x-Wert, bevor die Prädiktion startet\n",
    "# y_pos = y-Koordinaten Startwert = letzter gemessener y-Wert, bevor die Prädiktion startet\n",
    "def predict_next_n_steps(n, given_deltas, x_pos, y_pos):\n",
    "    pred_dx_values = [] # vorhergesagte Delta-x Werte\n",
    "    pred_dy_values = [] # vorhergesagte Delta-y Werte\n",
    "    pred_x_pos = [] # vorhergesagte x Koordinaten\n",
    "    pred_y_pos = [] # vorhergesagte y Koordinaten  \n",
    "    for i in range(0, n): \n",
    "        pred_value = model.predict(given_deltas) # nächsten Wert anhand der aktuellen 7 Deltas vorhersagen --> pred_value hat die Form [[dx dy]]\n",
    "        pred_value_unscaled = scaler.inverse_transform(pred_value) # Ergebnis zurückskalieren, um tatsächliche Delta-Werte zu erhalten\n",
    "        given_deltas = shift_deltas(given_deltas, pred_value) # Vorhergesagten Delta-Wert zur Liste für die nächste Prädiktion hinzufügen\n",
    "        pred_dx = round(pred_value_unscaled[0, 0], 3) # herausholen des vorhergesagten dx Wertes aus dem Array\n",
    "        pred_dy = round(pred_value_unscaled[0, 1], 3) # herausholen des vorhergesagten dy Wertes aus dem Array        \n",
    "        x_pos = round(x_pos + pred_dx, 3)\n",
    "        y_pos = round(y_pos + pred_dy, 3)\n",
    "        # berechneten Werte für dx, dy, xPos und yPos an die jeweilige Ergebnisliste anhängen\n",
    "        pred_dx_values.append(pred_dx)\n",
    "        pred_dy_values.append(pred_dy)\n",
    "        pred_x_pos.append(x_pos)\n",
    "        pred_y_pos.append(y_pos)\n",
    "    return pred_dx_values, pred_dy_values, pred_x_pos, pred_y_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c361a75a",
   "metadata": {},
   "source": [
    "Mithilfe der obigen Methoden können jetzt für jeden Fußgänger des Testdatensatzes die nächsten 12 dx und dy Werte, sowie die entsprechenden x- und y-Positionen vorhergesagt werden. Genau dies wird in folgendem Codeblock getätigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_range = 12 # Anzahl der Positionen, die vorhergesagt werden sollen\n",
    "\n",
    "predicted_dx_values = []\n",
    "predicted_dy_values = []\n",
    "predicted_x_pos = []\n",
    "predicted_y_pos = []\n",
    "\n",
    "ids_test =  np.array(df_test.ID.unique())\n",
    "progress = 1\n",
    "\n",
    "for id in ids_test: \n",
    "    print(\"Fortschritt: \", progress, \"/\", len(ids_test), end='\\r')\n",
    "    progress += 1\n",
    "    # Aktuellen Datenpunkt mithilfe der ID auswählen\n",
    "    current_datapoint = df_test[df_test.ID == id]\n",
    "    given_data = current_datapoint[current_datapoint.Timestamp <= 8] # ersten 8 Positionen sind gegeben, die nächsten 12 sollen vorhergesagt werden\n",
    "    \n",
    "    # anhängen der ersten 8 gegebenen Werte an die Trajektorien-Resultat-Listen\n",
    "    predicted_dx_values.extend(given_data['dx'].tolist())\n",
    "    predicted_dy_values.extend(given_data['dy'].tolist())\n",
    "    predicted_x_pos.extend(given_data['X'].tolist())\n",
    "    predicted_y_pos.extend(given_data['Y'].tolist())\n",
    "    \n",
    "    # von den gegebenen 8 Punkten die 7 Deltas extrahieren und basierend auf diesen die Vorhersagen machen\n",
    "    given_deltas = given_data.drop(given_data[given_data.Timestamp == 1].index) # ersten Delta-Wert für die vorhersage entfernen (da dieser sowieso immer 0 ist)\n",
    "    given_deltas = np.array(scaler.transform(given_deltas[['dx', 'dy']])) # skalieren der Daten + Umwandeln in Numpy-Array\n",
    "    given_deltas = np.reshape(given_deltas, (1, 7, 2)) # daten in korrekte Form bringen, damit man sie in das NN einfütter kann\n",
    "    \n",
    "    # Positionen des Fußgängers holen, an der er sich am Ende der beobachteten Zeit befindet --> = Startkoordinaten für die Prädiktion\n",
    "    start_x = predicted_x_pos[-1]\n",
    "    start_y = predicted_y_pos[-1]\n",
    "    \n",
    "    # nächsten n Positionen vorhersagen\n",
    "    pred_dx, pred_dy, pred_x, pred_y = predict_next_n_steps(forecast_range, given_deltas, start_x, start_y)\n",
    "    \n",
    "    predicted_dx_values.extend(pred_dx)\n",
    "    predicted_dy_values.extend(pred_dy)\n",
    "    predicted_x_pos.extend(pred_x)\n",
    "    predicted_y_pos.extend(pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57f959",
   "metadata": {},
   "source": [
    "Die vorhergesagten Ergebnisse für die Delta-Werte, sowie die vorhergesagten x- und y-Positionen werden an das DataFrame der Testdaten angehängt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d66b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Listen als Spalten an das Test-Dataframe anhängen\n",
    "df_test[\"predicted_dx\"] = predicted_dx_values\n",
    "df_test[\"predicted_dy\"] = predicted_dy_values\n",
    "df_test[\"predicted_x\"] = predicted_x_pos\n",
    "df_test[\"predicted_y\"] = predicted_y_pos\n",
    "print(df_test.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52231167",
   "metadata": {},
   "source": [
    "# 4. Bewertung der Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0b42b",
   "metadata": {},
   "source": [
    "Zur Bewertung der Performance unseres trainierten neuronalen Netzes werden die zwei in der Literatur gängigen Metriken \"**Average Displacement Error (ADE)**\" und \"**Final Displacement Error (FDE)**\" herangezogen. Die zwei Metriken lassen sich mit derselben Funktion berechnen, der einzige Unterschied liegt in den übergebenen Daten für die Berechnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cf66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnen des Displacement Errors\n",
    "# true_pos und pred_pos = Tatsächliche Positionen bzw. vorhergesagte Positionen | Numpy-Arrays der Form [[x1 y1],[x2 y2], ...]\n",
    "def calc_displacement_error(true_pos, pred_pos):\n",
    "    if (len(true_pos) != len(pred_pos)):\n",
    "        print(\"Listen haben unterschiedliche Länge, Displacement Error kann nicht berechnet werden!\")\n",
    "        return\n",
    "    n = len(true_pos) # Datenanzahl\n",
    "    sum_of_euclid_distances = 0 # Summe der euklidischen Distanzen der Punkte\n",
    "    for i in range (0, n):\n",
    "        sum_of_euclid_distances += np.linalg.norm(true_pos[i] - pred_pos[i])\n",
    "    return sum_of_euclid_distances / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6b51e",
   "metadata": {},
   "source": [
    "## 4.1 Average Displacement Error (ADE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1962841",
   "metadata": {},
   "source": [
    "Der ADE ist der durchschnittliche Fehler zwischen der Ground Trouth und der vorhergesagten Trajektorie aller vorhergesagten Positionen (in unserem Fall alle Timestamps > 8) über alle Fußgänger. Das bedeutet, wir holen uns alle Daten mit Timestamp > 8 aus dem Dataframe, da alle Positionen der Timestamps 9 bis 20 vorhergesagt wurden (basierend auf den ersten 8).\n",
    "\n",
    "Schöne Definition: https://arxiv.org/pdf/1907.08752.pdf#:~:text=Average%20Displacement%20Error%20(ADE)%3A,entire%20duration%20of%205%20seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abead7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df_test.drop(df_test[df_test.Timestamp < 9].index) # nur die vorhergesagten Daten auswählen\n",
    "true_positions = np.array(df_predictions[['X', 'Y']]) # Ground Trouth der Positionen\n",
    "predicted_positions = np.array(df_predictions[['predicted_x', 'predicted_y']]) # Vorhergesagte Positionen\n",
    "ade = calc_displacement_error(true_positions, predicted_positions)\n",
    "print(\"Average Displacement Error (ADE): \", round(ade, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce3777",
   "metadata": {},
   "source": [
    "## 4.2 Final Displacement Error (FDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5920a2",
   "metadata": {},
   "source": [
    "Der FDE ist der Fehler zwischen der finalen vorhergesagten Position, und der zugehörigen Ground Trouth. In anderen Worten: FDE = die Summe der euklidischen Distanzen zwischen allen final vorhergesagten Positionen vs. Ground Trouth geteilt durch die Anzahl der Fußgänger. Hier wählen wir also alle Daten mit Timestamp = 20 aus und übergeben sie der *calc_displacement_error* Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ce4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_positions = df_test[df_test.Timestamp == 20] # Daten für die finalen Positionen extrahieren \n",
    "true_final_positions = np.array(df_final_positions[['X', 'Y']]) # Ground Trouth der finalen Positionen\n",
    "predicted_final_positions = np.array(df_final_positions[['predicted_x', 'predicted_y']]) # Vorhergesagte finale Positionen\n",
    "fde = calc_displacement_error(true_final_positions, predicted_final_positions)\n",
    "print(\"Final Displacement Error (FDE): \", round(fde, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52481a",
   "metadata": {},
   "source": [
    "# 5. Zusätze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37addbcc",
   "metadata": {},
   "source": [
    "Unter diesem Unterpunkt sind zusätzliche Visualisierungen zu finden, welche sich teilweise auch in der Projektdokumentation wiederfinden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83766d5",
   "metadata": {},
   "source": [
    "## 5.1 Plotten des Displacement-Errors für jeden Zeitschritt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a510cf",
   "metadata": {},
   "source": [
    "Das trainierte NN hat iterativ die nächsten 12 Positionen vorhergesagt. Durch dieses iterative Vorgehen, summiert sich der Fehler immer mehr auf, da wir ja basierend auf einer evtl. falschen Vorhersage wieder eine erneute Vorhersage treffen. Wenn man das ganze dann 12 mal wiederholt, kann sich der Fehler ganz schön aufsummieren, was auch folgende Grafik verdeutlicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = []\n",
    "displacement_errors = []\n",
    "for timestep in range(9, 21): # Über alle vorhergesagten Positionen iterieren\n",
    "    df_current_timestep = df_test[df_test.Timestamp == timestep] # Daten des aktuellen Timestamps extrahieren\n",
    "    true_positions = np.array(df_current_timestep[['X', 'Y']]) # Ground Trouth der Positionen\n",
    "    predicted_positions = np.array(df_current_timestep[['predicted_x', 'predicted_y']]) # Vorhergesagte Positionen\n",
    "    de = calc_displacement_error(true_positions, predicted_positions) # Displacement Error für einen bestimmten Timestamp\n",
    "    timesteps.append(timestep)\n",
    "    displacement_errors.append(de)\n",
    "\n",
    "plot_displacement_errors(timesteps, displacement_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ec77b",
   "metadata": {},
   "source": [
    "## 5.2 Plotten von Beispiel-Fußgänger-Trajektorien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b9a4c",
   "metadata": {},
   "source": [
    "Hier kann man sich die beobachteten Positionen, sowie die gewünschten vorhergesagten Positionen der Fußgänger **aus einem gewünschten Datensatz** anzeigen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_dataset = df_train\n",
    "ids = np.array(desired_dataset.ID.unique())\n",
    "ids = ids[0:2] # Auswählen, welche Datenpunkte geplottet werden sollen\n",
    "for id in ids:\n",
    "    print(\"Fußgänger-ID: \", id, \"\\n\")\n",
    "    current_data = df_train[df_train.ID == id]\n",
    "    observed = current_data.loc[current_data.Timestamp <= 8] \n",
    "    predicted = current_data.loc[current_data.Timestamp > 8] \n",
    "    observed_x = observed['X'].tolist()\n",
    "    observed_y = observed['Y'].tolist()\n",
    "    predicted_x = predicted['X'].tolist()\n",
    "    predicted_y = predicted['Y'].tolist()\n",
    "    plot_pedestrian_trajectory(x_observed = observed_x, y_observed = observed_y, x_pred = predicted_x, y_pred = predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa9a61",
   "metadata": {},
   "source": [
    "## 5.3 Plotten von tatsächlich vorhergesagten Trajektorien vs. Ground Trouth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f2e54",
   "metadata": {},
   "source": [
    "Hier kann man sich die vom NN vorhergesagten Trajektorien **des Testdatensatzes**, sowie die zugehörige Ground Trouth visualisieren lassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(df_test.ID.unique())\n",
    "ids = ids[100:200] # Auswählen, welche Datenpunkte geplottet werden sollen\n",
    "for id in ids:\n",
    "    print(\"Fußgänger-ID: \", id, \"\\n\")\n",
    "    current_data = df_test[df_test.ID == id]\n",
    "    observed = current_data.loc[current_data.Timestamp <= 8] \n",
    "    predicted = current_data.loc[current_data.Timestamp > 8] \n",
    "    observed_x = observed['X'].tolist()\n",
    "    observed_y = observed['Y'].tolist()\n",
    "    gt_predicted_x = predicted['X'].tolist()\n",
    "    gt_predicted_y = predicted['Y'].tolist()\n",
    "    predicted_x = predicted['predicted_x'].tolist()\n",
    "    predicted_y = predicted['predicted_y'].tolist()\n",
    "    plot_pedestrian_trajectory(x_observed = observed_x, y_observed = observed_y, x_pred = predicted_x, y_pred = predicted_y, x_real = gt_predicted_x, y_real = gt_predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2665c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
